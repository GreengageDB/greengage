-- Tests EXPLAIN format output
-- ignore the variable JIT gucs and "optimizer = 'off'" in Settings (unaligned mode + text format)
-- start_matchsubs
-- m/^Settings:.*/
-- s/,?\s*optimizer_jit\w*\s*=\s*[^,\n]+//g
-- m/^Settings:.*/
-- s/,?\s*jit\w*\s*=\s*[^,\n]+//g
-- m/^Settings:.*/
-- s/^Settings:[,\s]*/Settings: /
-- m/^Settings:.*/
-- s/,?\s*optimizer\w*\s*=\s*'off'//g
-- end_matchsubs
-- ignore variable JIT gucs which can be shown when SETTINGS=ON
-- start_matchignore
-- m/^\s+jit\w*:/
-- m/\s*optimizer:\s*"off"/
-- m/^\s+optimizer_jit\w*:/
-- end_matchignore
-- To produce stable regression test output, it's usually necessary to
-- ignore details such as exact costs or row counts.  These filter
-- functions replace changeable output details with fixed strings.
create function explain_filter(text) returns setof text
language plpgsql as
$$
declare
    ln text;
begin
    for ln in execute $1
    loop
        -- Replace any numeric word with just 'N'
        ln := regexp_replace(ln, '-?\m\d+\M', 'N', 'g');
        -- In sort output, the above won't match units-suffixed numbers
        ln := regexp_replace(ln, '\m\d+kB', 'NkB', 'g');
        ln := regexp_replace(ln, '\m\d+K', 'NK', 'g');
        -- Replace slice and segment numbers with 'N'
        ln := regexp_replace(ln, '\mslice\d+', 'sliceN', 'g');
        ln := regexp_replace(ln, '\mseg\d+', 'segN', 'g');
        -- Ignore text-mode buffers output because it varies depending
        -- on the system state
        CONTINUE WHEN (ln ~ ' +Buffers: .*');
        -- Ignore text-mode "Planning:" line because whether it's output
        -- varies depending on the system state
        CONTINUE WHEN (ln = 'Planning:');
        return next ln;
    end loop;
end;
$$;
-- To produce valid JSON output, replace numbers with "0" or "0.0" not "N"
create function explain_filter_to_json(text) returns jsonb
language plpgsql as
$$
declare
    data text := '';
    ln text;
begin
    for ln in execute $1
    loop
        -- Replace any numeric word with just '0'
        ln := regexp_replace(ln, '\m\d+\M', '0', 'g');
        data := data || ln;
    end loop;
    return data::jsonb;
end;
$$;
-- DEFAULT syntax
CREATE TABLE apples(id int PRIMARY KEY, type text);
INSERT INTO apples(id) SELECT generate_series(1, 100000);
CREATE TABLE box_locations(id int PRIMARY KEY, address text);
CREATE TABLE boxes(id int PRIMARY KEY, apple_id int REFERENCES apples(id), location_id int REFERENCES box_locations(id));
WARNING:  referential integrity (FOREIGN KEY) constraints are not supported in Greengage Database, will not be enforced
WARNING:  referential integrity (FOREIGN KEY) constraints are not supported in Greengage Database, will not be enforced
--- Check Explain Text format output
select explain_filter('EXPLAIN SELECT * from boxes LEFT JOIN apples ON apples.id = boxes.apple_id LEFT JOIN box_locations ON box_locations.id = boxes.location_id;');
                                             explain_filter                                             
--------------------------------------------------------------------------------------------------------
 Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N)
   ->  Hash Left Join  (cost=N.N..N.N rows=N width=N)
         Hash Cond: (boxes.location_id = box_locations.id)
         ->  Redistribute Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N)
               Hash Key: boxes.location_id
               ->  Hash Left Join  (cost=N.N..N.N rows=N width=N)
                     Hash Cond: (boxes.apple_id = apples.id)
                     ->  Redistribute Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N)
                           Hash Key: boxes.apple_id
                           ->  Seq Scan on boxes  (cost=N.N..N.N rows=N width=N)
                     ->  Hash  (cost=N.N..N.N rows=N width=N)
                           ->  Seq Scan on apples  (cost=N.N..N.N rows=N width=N)
         ->  Hash  (cost=N.N..N.N rows=N width=N)
               ->  Seq Scan on box_locations  (cost=N.N..N.N rows=N width=N)
 Optimizer: Postgres-based planner
(15 rows)

--- Check Explain Analyze Text output that include the slices information
select explain_filter('EXPLAIN (ANALYZE) SELECT * from boxes LEFT JOIN apples ON apples.id = boxes.apple_id LEFT JOIN box_locations ON box_locations.id = boxes.location_id;');
                                                                explain_filter                                                                
----------------------------------------------------------------------------------------------------------------------------------------------
 Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
   ->  Hash Left Join  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
         Hash Cond: (boxes.location_id = box_locations.id)
         ->  Redistribute Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
               Hash Key: boxes.location_id
               ->  Hash Left Join  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
                     Hash Cond: (boxes.apple_id = apples.id)
                     ->  Redistribute Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
                           Hash Key: boxes.apple_id
                           ->  Seq Scan on boxes  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
                     ->  Hash  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
                           Buckets: N  Batches: N  Memory Usage: NkB
                           ->  Seq Scan on apples  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
         ->  Hash  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
               Buckets: N  Batches: N  Memory Usage: NkB
               ->  Seq Scan on box_locations  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
 Optimizer: Postgres-based planner
 Planning Time: N.N ms
   (sliceN)    Executor memory: NK bytes.
   (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).  Work_mem: NK bytes max.
   (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).  Work_mem: NK bytes max.
   (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
 Memory used:  NkB
 Execution Time: N.N ms
(24 rows)

-- Unaligned output format is better for the YAML / XML / JSON outputs.
-- In aligned format, you have end-of-line markers at the end of each line,
-- and its position depends on the longest line. If the width changes, all
-- lines need to be adjusted for the moved end-of-line-marker.
\a
-- YAML Required replaces for costs and time changes
-- Check Explain YAML output
select explain_filter('EXPLAIN (FORMAT YAML) SELECT * from boxes LEFT JOIN apples ON apples.id = boxes.apple_id LEFT JOIN box_locations ON box_locations.id = boxes.location_id;');
explain_filter
- Plan: 
    Node Type: "Gather Motion"
    Senders: N
    Receivers: N
    Slice: N
    Segments: N
    Gang Type: "primary reader"
    Parallel Aware: false
    Startup Cost: N.N
    Total Cost: N.N
    Plan Rows: N
    Plan Width: N
    Plans: 
      - Node Type: "Hash Join"
        Parent Relationship: "Outer"
        Slice: N
        Segments: N
        Gang Type: "primary reader"
        Parallel Aware: false
        Join Type: "Left"
        Startup Cost: N.N
        Total Cost: N.N
        Plan Rows: N
        Plan Width: N
        Inner Unique: true
        Hash Cond: "(boxes.location_id = box_locations.id)"
        Plans: 
          - Node Type: "Redistribute Motion"
            Senders: N
            Receivers: N
            Parent Relationship: "Outer"
            Slice: N
            Segments: N
            Gang Type: "primary reader"
            Parallel Aware: false
            Startup Cost: N.N
            Total Cost: N.N
            Plan Rows: N
            Plan Width: N
            Hash Key: "boxes.location_id"
            Plans: 
              - Node Type: "Hash Join"
                Parent Relationship: "Outer"
                Slice: N
                Segments: N
                Gang Type: "primary reader"
                Parallel Aware: false
                Join Type: "Left"
                Startup Cost: N.N
                Total Cost: N.N
                Plan Rows: N
                Plan Width: N
                Inner Unique: true
                Hash Cond: "(boxes.apple_id = apples.id)"
                Plans: 
                  - Node Type: "Redistribute Motion"
                    Senders: N
                    Receivers: N
                    Parent Relationship: "Outer"
                    Slice: N
                    Segments: N
                    Gang Type: "primary reader"
                    Parallel Aware: false
                    Startup Cost: N.N
                    Total Cost: N.N
                    Plan Rows: N
                    Plan Width: N
                    Hash Key: "boxes.apple_id"
                    Plans: 
                      - Node Type: "Seq Scan"
                        Parent Relationship: "Outer"
                        Slice: N
                        Segments: N
                        Gang Type: "primary reader"
                        Parallel Aware: false
                        Relation Name: "boxes"
                        Alias: "boxes"
                        Startup Cost: N.N
                        Total Cost: N.N
                        Plan Rows: N
                        Plan Width: N
                  - Node Type: "Hash"
                    Parent Relationship: "Inner"
                    Slice: N
                    Segments: N
                    Gang Type: "primary reader"
                    Parallel Aware: false
                    Startup Cost: N.N
                    Total Cost: N.N
                    Plan Rows: N
                    Plan Width: N
                    Plans: 
                      - Node Type: "Seq Scan"
                        Parent Relationship: "Outer"
                        Slice: N
                        Segments: N
                        Gang Type: "primary reader"
                        Parallel Aware: false
                        Relation Name: "apples"
                        Alias: "apples"
                        Startup Cost: N.N
                        Total Cost: N.N
                        Plan Rows: N
                        Plan Width: N
          - Node Type: "Hash"
            Parent Relationship: "Inner"
            Slice: N
            Segments: N
            Gang Type: "primary reader"
            Parallel Aware: false
            Startup Cost: N.N
            Total Cost: N.N
            Plan Rows: N
            Plan Width: N
            Plans: 
              - Node Type: "Seq Scan"
                Parent Relationship: "Outer"
                Slice: N
                Segments: N
                Gang Type: "primary reader"
                Parallel Aware: false
                Relation Name: "box_locations"
                Alias: "box_locations"
                Startup Cost: N.N
                Total Cost: N.N
                Plan Rows: N
                Plan Width: N
  Optimizer: "Postgres-based planner"
(1 row)
SET random_page_cost = 1;
SET cpu_index_tuple_cost = 0.1;
select explain_filter('EXPLAIN (FORMAT YAML, VERBOSE) SELECT * from boxes;');
explain_filter
- Plan: 
    Node Type: "Gather Motion"
    Senders: N
    Receivers: N
    Slice: N
    Segments: N
    Gang Type: "primary reader"
    Parallel Aware: false
    Startup Cost: N.N
    Total Cost: N.N
    Plan Rows: N
    Plan Width: N
    Output: 
      - "id"
      - "apple_id"
      - "location_id"
    Plans: 
      - Node Type: "Seq Scan"
        Parent Relationship: "Outer"
        Slice: N
        Segments: N
        Gang Type: "primary reader"
        Parallel Aware: false
        Relation Name: "boxes"
        Schema: "public"
        Alias: "boxes"
        Startup Cost: N.N
        Total Cost: N.N
        Plan Rows: N
        Plan Width: N
        Output: 
          - "id"
          - "apple_id"
          - "location_id"
  Optimizer: "Postgres-based planner"
  Settings: 
    cpu_index_tuple_cost: "N.N"
    optimizer: "off"
    random_page_cost: "N"
(1 row)
select explain_filter('EXPLAIN (FORMAT YAML, VERBOSE, SETTINGS ON) SELECT * from boxes;');
explain_filter
- Plan: 
    Node Type: "Gather Motion"
    Senders: N
    Receivers: N
    Slice: N
    Segments: N
    Gang Type: "primary reader"
    Parallel Aware: false
    Startup Cost: N.N
    Total Cost: N.N
    Plan Rows: N
    Plan Width: N
    Output: 
      - "id"
      - "apple_id"
      - "location_id"
    Plans: 
      - Node Type: "Seq Scan"
        Parent Relationship: "Outer"
        Slice: N
        Segments: N
        Gang Type: "primary reader"
        Parallel Aware: false
        Relation Name: "boxes"
        Schema: "public"
        Alias: "boxes"
        Startup Cost: N.N
        Total Cost: N.N
        Plan Rows: N
        Plan Width: N
        Output: 
          - "id"
          - "apple_id"
          - "location_id"
  Optimizer: "Postgres-based planner"
  Settings: 
    cpu_index_tuple_cost: "N.N"
    random_page_cost: "N"
    optimizer: "off"
(1 row)
--- Check Explain Analyze YAML output that include the slices information
select explain_filter('EXPLAIN (ANALYZE, FORMAT YAML) SELECT * from boxes LEFT JOIN apples ON apples.id = boxes.apple_id LEFT JOIN box_locations ON box_locations.id = boxes.location_id;');
explain_filter
- Plan: 
    Node Type: "Gather Motion"
    Senders: N
    Receivers: N
    Slice: N
    Segments: N
    Gang Type: "primary reader"
    Parallel Aware: false
    Startup Cost: N.N
    Total Cost: N.N
    Plan Rows: N
    Plan Width: N
    Actual Startup Time: N.N
    Actual Total Time: N.N
    Actual Rows: N
    Actual Loops: N
    Plans: 
      - Node Type: "Hash Join"
        Parent Relationship: "Outer"
        Slice: N
        Segments: N
        Gang Type: "primary reader"
        Parallel Aware: false
        Join Type: "Left"
        Startup Cost: N.N
        Total Cost: N.N
        Plan Rows: N
        Plan Width: N
        Actual Startup Time: N.N
        Actual Total Time: N.N
        Actual Rows: N
        Actual Loops: N
        Inner Unique: true
        Hash Cond: "(boxes.location_id = box_locations.id)"
        Plans: 
          - Node Type: "Redistribute Motion"
            Senders: N
            Receivers: N
            Parent Relationship: "Outer"
            Slice: N
            Segments: N
            Gang Type: "primary reader"
            Parallel Aware: false
            Startup Cost: N.N
            Total Cost: N.N
            Plan Rows: N
            Plan Width: N
            Actual Startup Time: N.N
            Actual Total Time: N.N
            Actual Rows: N
            Actual Loops: N
            Hash Key: "boxes.location_id"
            Plans: 
              - Node Type: "Hash Join"
                Parent Relationship: "Outer"
                Slice: N
                Segments: N
                Gang Type: "primary reader"
                Parallel Aware: false
                Join Type: "Left"
                Startup Cost: N.N
                Total Cost: N.N
                Plan Rows: N
                Plan Width: N
                Actual Startup Time: N.N
                Actual Total Time: N.N
                Actual Rows: N
                Actual Loops: N
                Inner Unique: true
                Hash Cond: "(boxes.apple_id = apples.id)"
                Plans: 
                  - Node Type: "Redistribute Motion"
                    Senders: N
                    Receivers: N
                    Parent Relationship: "Outer"
                    Slice: N
                    Segments: N
                    Gang Type: "primary reader"
                    Parallel Aware: false
                    Startup Cost: N.N
                    Total Cost: N.N
                    Plan Rows: N
                    Plan Width: N
                    Actual Startup Time: N.N
                    Actual Total Time: N.N
                    Actual Rows: N
                    Actual Loops: N
                    Hash Key: "boxes.apple_id"
                    Plans: 
                      - Node Type: "Seq Scan"
                        Parent Relationship: "Outer"
                        Slice: N
                        Segments: N
                        Gang Type: "primary reader"
                        Parallel Aware: false
                        Relation Name: "boxes"
                        Alias: "boxes"
                        Startup Cost: N.N
                        Total Cost: N.N
                        Plan Rows: N
                        Plan Width: N
                        Actual Startup Time: N.N
                        Actual Total Time: N.N
                        Actual Rows: N
                        Actual Loops: N
                  - Node Type: "Hash"
                    Parent Relationship: "Inner"
                    Slice: N
                    Segments: N
                    Gang Type: "primary reader"
                    Parallel Aware: false
                    Startup Cost: N.N
                    Total Cost: N.N
                    Plan Rows: N
                    Plan Width: N
                    Actual Startup Time: N.N
                    Actual Total Time: N.N
                    Actual Rows: N
                    Actual Loops: N
                    Hash Buckets: N
                    Original Hash Buckets: N
                    Hash Batches: N
                    Original Hash Batches: N
                    Peak Memory Usage: N
                    Plans: 
                      - Node Type: "Seq Scan"
                        Parent Relationship: "Outer"
                        Slice: N
                        Segments: N
                        Gang Type: "primary reader"
                        Parallel Aware: false
                        Relation Name: "apples"
                        Alias: "apples"
                        Startup Cost: N.N
                        Total Cost: N.N
                        Plan Rows: N
                        Plan Width: N
                        Actual Startup Time: N.N
                        Actual Total Time: N.N
                        Actual Rows: N
                        Actual Loops: N
          - Node Type: "Hash"
            Parent Relationship: "Inner"
            Slice: N
            Segments: N
            Gang Type: "primary reader"
            Parallel Aware: false
            Startup Cost: N.N
            Total Cost: N.N
            Plan Rows: N
            Plan Width: N
            Actual Startup Time: N.N
            Actual Total Time: N.N
            Actual Rows: N
            Actual Loops: N
            Hash Buckets: N
            Original Hash Buckets: N
            Hash Batches: N
            Original Hash Batches: N
            Peak Memory Usage: N
            Plans: 
              - Node Type: "Seq Scan"
                Parent Relationship: "Outer"
                Slice: N
                Segments: N
                Gang Type: "primary reader"
                Parallel Aware: false
                Relation Name: "box_locations"
                Alias: "box_locations"
                Startup Cost: N.N
                Total Cost: N.N
                Plan Rows: N
                Plan Width: N
                Actual Startup Time: N.N
                Actual Total Time: N.N
                Actual Rows: N
                Actual Loops: N
  Optimizer: "Postgres-based planner"
  Planning Time: N.N
  Triggers: 
  Slice statistics: 
    - Slice: N
      Executor Memory: N
    - Slice: N
      Executor Memory: 
        Average: N
        Workers: N
        Maximum Memory Used: N
      Work Maximum Memory: N
    - Slice: N
      Executor Memory: 
        Average: N
        Workers: N
        Maximum Memory Used: N
      Work Maximum Memory: N
    - Slice: N
      Executor Memory: 
        Average: N
        Workers: N
        Maximum Memory Used: N
  Statement statistics: 
    Memory used: N
  Execution Time: N.N
(1 row)
--- Check explain analyze sort information in verbose mode
select explain_filter('EXPLAIN (ANALYZE, VERBOSE) SELECT * from boxes ORDER BY apple_id;');
explain_filter
Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
  Output: id, apple_id, location_id
  Merge Key: apple_id
  ->  Sort  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
        Output: id, apple_id, location_id
        Sort Key: boxes.apple_id
        Sort Method:  quicksort  Memory: NkB  Max Memory: NkB  Avg Memory: NkB (N segments)
        Executor Memory: NkB  Segments: N  Max: NkB (segment N)
        work_mem: NkB  Segments: N  Max: NkB (segment N)  Workfile: (N spilling)
        ->  Seq Scan on public.boxes  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
              Output: id, apple_id, location_id
Optimizer: Postgres-based planner
Settings: cpu_index_tuple_cost = 'N.N', optimizer = 'off', random_page_cost = 'N'
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).  Work_mem: NK bytes max.
Memory used:  NkB
Execution Time: N.N ms
(18 rows)
RESET random_page_cost;
RESET cpu_index_tuple_cost;
--
-- Test a simple case with JSON and XML output, too.
--
-- This should be enough for those format. The only difference between JSON,
-- XML, and YAML is in the formatting, after all.
-- Check JSON format
select explain_filter_to_json('EXPLAIN (FORMAT JSON, COSTS OFF) SELECT * FROM generate_series(1, 10);');
explain_filter_to_json
[{"Plan": {"Alias": "generate_series", "Slice": 0, "Segments": 0, "Gang Type": "unallocated", "Node Type": "Function Scan", "Function Name": "generate_series", "Parallel Aware": false}, "Optimizer": "Postgres-based planner"}]
(1 row)
select explain_filter('EXPLAIN (FORMAT XML, COSTS OFF) SELECT * FROM generate_series(1, 10);');
explain_filter
<explain xmlns="http://www.postgresql.org/N/explain">
  <Query>
    <Plan>
      <Node-Type>Function Scan</Node-Type>
      <Slice>N</Slice>
      <Segments>N</Segments>
      <Gang-Type>unallocated</Gang-Type>
      <Parallel-Aware>false</Parallel-Aware>
      <Function-Name>generate_series</Function-Name>
      <Alias>generate_series</Alias>
    </Plan>
    <Optimizer>Postgres-based planner</Optimizer>
  </Query>
</explain>
(1 row)
-- Test for an old bug in printing Sequence nodes in JSON/XML format
-- (https://github.com/GreengageDB/greengage/issues/9410)
CREATE TABLE jsonexplaintest (i int4) PARTITION BY RANGE (i) (START(1) END(3) EVERY(1));
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'i' as the Greengage Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
select explain_filter_to_json('EXPLAIN (FORMAT JSON, COSTS OFF) SELECT * FROM jsonexplaintest WHERE i = 2;');
explain_filter_to_json
[{"Plan": {"Plans": [{"Alias": "jsonexplaintest_1_prt_2", "Slice": 0, "Filter": "(i = 0)", "Segments": 0, "Gang Type": "primary reader", "Node Type": "Seq Scan", "Relation Name": "jsonexplaintest_1_prt_2", "Parallel Aware": false, "Parent Relationship": "Outer"}], "Slice": 0, "Senders": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Gather Motion", "Receivers": 0, "Parallel Aware": false}, "Optimizer": "Postgres-based planner"}]
(1 row)
-- Check Explain Text format output with jit enable
CREATE TABLE jit_explain_output(c1 int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c1' as the Greengage Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
INSERT INTO jit_explain_output SELECT generate_series(1,100);
SET jit = on;
SET jit_above_cost = 0;
SET gp_explain_jit = on;
-- ORCA GUCs to enable JIT
set optimizer_jit_above_cost to 0;
select explain_filter('EXPLAIN SELECT * FROM jit_explain_output LIMIT 10;');
explain_filter
Limit  (cost=N.N..N.N rows=N width=N)
  ->  Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N)
        ->  Limit  (cost=N.N..N.N rows=N width=N)
              ->  Seq Scan on jit_explain_output  (cost=N.N..N.N rows=N width=N)
Optimizer: Postgres-based planner
JIT:
  Functions: N
  Options: Inlining false, Optimization false, Expressions true, Deforming true
(8 rows)
-- Check explain anlyze text format output with jit enable
select explain_filter('EXPLAIN (ANALYZE) SELECT * FROM jit_explain_output LIMIT 10;');
explain_filter
Limit  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
  ->  Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
        ->  Limit  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
              ->  Seq Scan on jit_explain_output  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
Optimizer: Postgres-based planner
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
Memory used:  NkB
JIT:
  Options: Inlining false, Optimization false, Expressions true, Deforming true.
  (sliceN): Functions: N.N. Timing: N.N ms total.
  (sliceN): Functions: N.N avg x N workers, N.N max (segN). Timing: N.N ms avg x N workers, N.N ms max (segN).
Execution Time: N.N ms
(14 rows)
-- Check explain analyze json format output with jit enable
-- JSON Required replaces for costs and time changes
-- start_matchsubs
-- m/ "Startup Cost": \d+.\d+/
-- s/ "Startup Cost": \d+.\d+/ "Startup Cost": ##.###/g
-- m/ "Total Cost": \d+.\d+/
-- s/ "Total Cost": \d+.\d+/ "Total Cost": ##.###/g
-- m/ "Plan Rows": \d+/
-- s/ "Plan Rows": \d+/ "Plan Rows": #####/g
-- m/ "Actual Startup Time": \d+.\d+/
-- s/ "Actual Startup Time": \d+.\d+/ "Actual Startup Time": ##.###/g
-- m/ "Actual Total Time": \d+.\d+/
-- s/ "Actual Total Time": \d+.\d+/ "Actual Total Time": ##.###/g
-- m/ "Time To First Result": "\d+(.\d+)?"/
-- s/ "Time To First Result": "\d+(.\d+)?"/ "Time To First Result": "##.###"/g
-- m/ "Time To Total Result": "\d+(.\d+)?"/
-- s/ "Time To Total Result": "\d+(.\d+)?"/ "Time To Total Result": "##.###"/g
-- m/ "Planning Time": \d+.\d+/
-- s/ "Planning Time": \d+.\d+/ "Planning Time": ##.###/g
-- m/ "Executor Memory": \d+/
-- s/ "Executor Memory": \d+/ "Executor Memory": #####/g
-- m/ "Average": \d+/
-- s/ "Average": \d+/ "Average": #####/g
-- m/ "Maximum Memory Used": \d+/
-- s/ "Maximum Memory Used": \d+/ "Maximum Memory Used": #####/g
-- m/ "Memory used": \d+/
-- s/ "Memory used": \d+/ "Memory used": #####/g
-- m/ "Execution Time": \d+.\d+/
-- s/ "Execution Time": \d+.\d+/ "Execution Time": ##.###/g
-- end_matchsubs
select explain_filter_to_json('EXPLAIN (ANALYZE, FORMAT json) SELECT * FROM jit_explain_output LIMIT 10;');
explain_filter_to_json
[{"JIT": {"slice": {"slice": 0, "Timing": {"avg": 0.0, "max": 0.0, "segid": 0, "nworker": 0}, "Functions": {"avg": 0.0, "max": 0.0, "segid": 0, "nworker": 0}}, "Options": {"Inlining": false, "Deforming": true, "Expressions": true, "Optimization": false}}, "Plan": {"Plans": [{"Plans": [{"Plans": [{"Alias": "jit_explain_output", "Slice": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Seq Scan", "Plan Rows": 0, "Plan Width": 0, "Total Cost": 0.0, "Actual Rows": 0, "Actual Loops": 0, "Startup Cost": 0.0, "Relation Name": "jit_explain_output", "Parallel Aware": false, "Actual Total Time": 0.0, "Actual Startup Time": 0.0, "Parent Relationship": "Outer"}], "Slice": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Limit", "Plan Rows": 0, "Plan Width": 0, "Total Cost": 0.0, "Actual Rows": 0, "Actual Loops": 0, "Startup Cost": 0.0, "Parallel Aware": false, "Actual Total Time": 0.0, "Actual Startup Time": 0.0, "Parent Relationship": "Outer"}], "Slice": 0, "Senders": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Gather Motion", "Plan Rows": 0, "Receivers": 0, "Plan Width": 0, "Total Cost": 0.0, "Actual Rows": 0, "Actual Loops": 0, "Startup Cost": 0.0, "Parallel Aware": false, "Actual Total Time": 0.0, "Actual Startup Time": 0.0, "Parent Relationship": "Outer"}], "Slice": 0, "Segments": 0, "Gang Type": "unallocated", "Node Type": "Limit", "Plan Rows": 0, "Plan Width": 0, "Total Cost": 0.0, "Actual Rows": 0, "Actual Loops": 0, "Startup Cost": 0.0, "Parallel Aware": false, "Actual Total Time": 0.0, "Actual Startup Time": 0.0}, "Triggers": [], "Optimizer": "Postgres-based planner", "Planning Time": 0.0, "Execution Time": 0.0, "Slice statistics": [{"Slice": 0, "Executor Memory": 0}, {"Slice": 0, "Executor Memory": {"Average": 0, "Workers": 0, "Maximum Memory Used": 0}}], "Statement statistics": {"Memory used": 0}}]
(1 row)
RESET jit;
RESET jit_above_cost;
RESET gp_explain_jit;
RESET optimizer_jit_above_cost;
-- Greengage hash table extra message
CREATE TABLE test_src_tbl AS
SELECT i % 10000 AS a, i % 10000 + 1 AS b FROM generate_series(1, 50000) i DISTRIBUTED BY (a);
ANALYZE test_src_tbl;
-- Enable optimizer_enable_hashagg, and set statement_mem to a small value to force spilling
set optimizer_enable_hashagg = on;
SET statement_mem = '1000kB';
-- Hashagg with spilling
CREATE TABLE test_hashagg_spill AS
SELECT a, COUNT(DISTINCT b) AS b FROM test_src_tbl GROUP BY a;
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column(s) named 'a' as the Greengage Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
select explain_filter('EXPLAIN ANALYZE SELECT a, COUNT(DISTINCT b) AS b FROM test_src_tbl GROUP BY a;');
explain_filter
Gather Motion N:N  (sliceN; segments: N)  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
  ->  HashAggregate  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
        Group Key: a
        Planned Partitions: N
        Extra Text: (segN)   hash table(s): N; N groups total in N batches, N spill partitions; disk usage: NKB; chain length N.N avg, N max; using N of N buckets; total N expansions.

        ->  HashAggregate  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
              Group Key: a, b
              Extra Text: (segN)   hash table(s): N; chain length N.N avg, N max; using N of N buckets; total N expansions.

              ->  Seq Scan on test_src_tbl  (cost=N.N..N.N rows=N width=N) (actual time=N.N..N.N rows=N loops=N)
Optimizer: Postgres-based planner
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
* (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).  Work_mem: NK bytes max, NK bytes wanted.
Memory used:  NkB
Memory wanted:  NkB
Execution Time: N.N ms
(18 rows)
-- Hashagg with grouping sets
CREATE TABLE test_hashagg_groupingsets AS
SELECT a, avg(b) AS b FROM test_src_tbl GROUP BY grouping sets ((a), (b));
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column(s) named 'a' as the Greengage Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
-- The planner generates multiple hash tables but ORCA uses Shared Scan.
WITH query_plan (et) AS
(
	SELECT explain_filter(
		'EXPLAIN ANALYZE SELECT a, avg(b) AS b FROM test_src_tbl GROUP BY grouping sets ((a), (b));')
)
SELECT BTRIM(et) as explain_info FROM query_plan WHERE et like '%Extra Text%' limit 2;
explain_info
Extra Text: (segN)   hash table(s): N; N groups total in N batches, N spill partitions; disk usage: NKB; chain length N.N avg, N max; using N of N buckets; total N expansions.
Extra Text: (segN)   hash table(s): N; N groups total in N batches, N spill partitions; disk usage: NKB; chain length N.N avg, N max; using N of N buckets; total N expansions.
(2 rows)
RESET optimizer_enable_hashagg;
RESET statement_mem;
-- Check EXPLAIN format output with BUFFERS enabled
-- Insert rows into a single segment
SET track_io_timing = on;
CREATE TABLE stat_io_timing(a, b) AS SELECT 0, i FROM generate_series(1, 10000) i DISTRIBUTED BY (a);
ANALYZE stat_io_timing;
-- explain_processing_off
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF)
SELECT a FROM stat_io_timing
WHERE b BETWEEN 5 AND 9;');
explain_filter
Gather Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
  ->  Seq Scan on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
        Filter: ((b >= N) AND (b <= N))
        Rows Removed by Filter: N
Optimizer: Postgres-based planner
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
Memory used:  NkB
Execution Time: N.N ms
(10 rows)
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF, SUMMARY OFF)
SELECT a FROM stat_io_timing
WHERE b BETWEEN 5 AND 9;');
explain_filter
Gather Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
  ->  Seq Scan on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
        Filter: ((b >= N) AND (b <= N))
        Rows Removed by Filter: N
Optimizer: Postgres-based planner
(5 rows)
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF)
INSERT INTO stat_io_timing (SELECT * FROM stat_io_timing);');
explain_filter
Insert on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
  ->  Seq Scan on stat_io_timing stat_io_timing_1 (actual time=N.N..N.N rows=N loops=N)
Optimizer: Postgres-based planner
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
Memory used:  NkB
Execution Time: N.N ms
(7 rows)
select explain_filter_to_json('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF, FORMAT JSON)
INSERT INTO stat_io_timing (SELECT * FROM stat_io_timing);');
explain_filter_to_json
[{"Plan": {"Alias": "stat_io_timing", "Plans": [{"Alias": "stat_io_timing_1", "Slice": 0, "Segments": 0, "Gang Type": "primary writer", "Node Type": "Seq Scan", "Actual Rows": 0, "Actual Loops": 0, "I/O Read Time": 0.0, "Relation Name": "stat_io_timing", "I/O Write Time": 0.0, "Parallel Aware": false, "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Parent Relationship": "Member", "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0}], "Slice": 0, "Segments": 0, "Gang Type": "primary writer", "Node Type": "ModifyTable", "Operation": "Insert", "Actual Rows": 0, "Actual Loops": 0, "I/O Read Time": 0.0, "Relation Name": "stat_io_timing", "I/O Write Time": 0.0, "Parallel Aware": false, "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0}, "Triggers": [], "Optimizer": "Postgres-based planner", "Planning Time": 0.0, "Execution Time": 0.0, "Slice statistics": [{"Slice": 0, "Executor Memory": {"Average": 0, "Workers": 0, "Maximum Memory Used": 0}}], "Statement statistics": {"Memory used": 0}}]
(1 row)
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF, SUMMARY OFF)
INSERT INTO stat_io_timing (SELECT * FROM stat_io_timing);');
explain_filter
Insert on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
  ->  Seq Scan on stat_io_timing stat_io_timing_1 (actual time=N.N..N.N rows=N loops=N)
Optimizer: Postgres-based planner
(3 rows)
select explain_filter_to_json('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF, FORMAT JSON)
SELECT b FROM stat_io_timing where b=50;');
explain_filter_to_json
[{"Plan": {"Plans": [{"Alias": "stat_io_timing", "Slice": 0, "Filter": "(b = 0)", "Segments": 0, "Gang Type": "primary reader", "Node Type": "Seq Scan", "Actual Rows": 0, "Actual Loops": 0, "I/O Read Time": 0.0, "Relation Name": "stat_io_timing", "I/O Write Time": 0.0, "Parallel Aware": false, "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Parent Relationship": "Outer", "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0, "Rows Removed by Filter": 0}], "Slice": 0, "Senders": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Gather Motion", "Receivers": 0, "Actual Rows": 0, "Actual Loops": 0, "I/O Read Time": 0.0, "I/O Write Time": 0.0, "Parallel Aware": false, "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0}, "Triggers": [], "Optimizer": "Postgres-based planner", "Planning Time": 0.0, "Execution Time": 0.0, "Slice statistics": [{"Slice": 0, "Executor Memory": 0}, {"Slice": 0, "Executor Memory": {"Average": 0, "Workers": 0, "Maximum Memory Used": 0}}], "Statement statistics": {"Memory used": 0}}]
(1 row)
CREATE INDEX stat_io_timing_idx ON stat_io_timing (b);
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF)
SELECT b FROM stat_io_timing where b=50;');
explain_filter
Gather Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
  ->  Index Only Scan using stat_io_timing_idx on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
        Index Cond: (b = N)
        Heap Fetches: N
        I/O Timings: read=N.N
Optimizer: Postgres-based planner
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
Memory used:  NkB
Execution Time: N.N ms
(11 rows)
select explain_filter_to_json('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF, FORMAT JSON)
SELECT b FROM stat_io_timing where b=50;');
explain_filter_to_json
[{"Plan": {"Plans": [{"Alias": "stat_io_timing", "Slice": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Index Only Scan", "Index Cond": "(b = 0)", "Index Name": "stat_io_timing_idx", "Actual Rows": 0, "Actual Loops": 0, "Heap Fetches": 0, "I/O Read Time": 0.0, "Relation Name": "stat_io_timing", "I/O Write Time": 0.0, "Parallel Aware": false, "Scan Direction": "Forward", "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Parent Relationship": "Outer", "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0, "Rows Removed by Index Recheck": 0}], "Slice": 0, "Senders": 0, "Segments": 0, "Gang Type": "primary reader", "Node Type": "Gather Motion", "Receivers": 0, "Actual Rows": 0, "Actual Loops": 0, "I/O Read Time": 0.0, "I/O Write Time": 0.0, "Parallel Aware": false, "Local Hit Blocks": 0, "Temp Read Blocks": 0, "Actual Total Time": 0.0, "Local Read Blocks": 0, "Shared Hit Blocks": 0, "Shared Read Blocks": 0, "Actual Startup Time": 0.0, "Temp Written Blocks": 0, "Local Dirtied Blocks": 0, "Local Written Blocks": 0, "Shared Dirtied Blocks": 0, "Shared Written Blocks": 0}, "Triggers": [], "Optimizer": "Postgres-based planner", "Planning Time": 0.0, "Execution Time": 0.0, "Slice statistics": [{"Slice": 0, "Executor Memory": 0}, {"Slice": 0, "Executor Memory": {"Average": 0, "Workers": 0, "Maximum Memory Used": 0}}], "Statement statistics": {"Memory used": 0}}]
(1 row)
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF)
SELECT s1.b FROM stat_io_timing s1 join stat_io_timing s2 on s1.b=s2.b where s1.a=50;');
explain_filter
Gather Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
  ->  Hash Join (actual time=N.N..N.N rows=N loops=N)
        Hash Cond: (s2.b = s1.b)
        ->  Seq Scan on stat_io_timing s2 (never executed)
        ->  Hash (actual time=N.N..N.N rows=N loops=N)
              Buckets: N  Batches: N  Memory Usage: NkB
              ->  Broadcast Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
                    ->  Seq Scan on stat_io_timing s1 (actual time=N.N..N.N rows=N loops=N)
                          Filter: (a = N)
Optimizer: Postgres-based planner
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).  Work_mem: NK bytes max.
  (sliceN)    Executor memory: NK bytes (segN).
Memory used:  NkB
Execution Time: N.N ms
(16 rows)
-- Test Bitmap Heap Scan block accounting
SET enable_seqscan = 0;
SET enable_bitmapscan = 1;
SET optimizer_enable_tablescan = 0;
SET optimizer_enable_bitmapscan = 1;
CREATE INDEX stat_io_timing_brin_idx ON stat_io_timing USING brin (b);
VACUUM ANALYZE stat_io_timing;
select explain_filter('EXPLAIN (ANALYZE, BUFFERS, COSTS OFF)
SELECT * FROM stat_io_timing WHERE b = 1;');
explain_filter
Gather Motion N:N  (sliceN; segments: N) (actual time=N.N..N.N rows=N loops=N)
  ->  Bitmap Heap Scan on stat_io_timing (actual time=N.N..N.N rows=N loops=N)
        Recheck Cond: (b = N)
        Heap Blocks: exact=N
        ->  Bitmap Index Scan on stat_io_timing_idx (actual time=N.N..N.N rows=N loops=N)
              Index Cond: (b = N)
Optimizer: Postgres-based planner
Planning Time: N.N ms
  (sliceN)    Executor memory: NK bytes.
  (sliceN)    Executor memory: NK bytes avg x N workers, NK bytes max (segN).
Memory used:  NkB
Execution Time: N.N ms
(12 rows)
RESET track_io_timing;
RESET enable_seqscan;
RESET enable_bitmapscan;
RESET optimizer_enable_tablescan;
RESET optimizer_enable_bitmapscan;
-- Test Allstats formatting
create table allstat_test(a int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greengage Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
set gp_enable_explain_allstat=true;
explain (analyze, format json) select * from allstat_test;
QUERY PLAN
[
  {
    "Plan": {
      "Node Type": "Gather Motion",
      "Senders": 3,
      "Receivers": 1,
      "Slice": 1,
      "Segments": 3,
      "Gang Type": "primary reader",
      "Parallel Aware": false,
      "Startup Cost": 0.00,
      "Total Cost": 1639.00,
      "Plan Rows": 96300,
      "Plan Width": 4,
      "Actual Startup Time": 0.242,
      "Actual Total Time": 0.242,
      "Actual Rows": 0,
      "Actual Loops": 1,
      "Allstat": [
        {
          "Segment index": -1,
          "Time To First Result": "0.109",
          "Time To Total Result": "0.242",
          "Tuples": 0.0
        }
      ],
      "Plans": [
        {
          "Node Type": "Seq Scan",
          "Parent Relationship": "Outer",
          "Slice": 1,
          "Segments": 3,
          "Gang Type": "primary reader",
          "Parallel Aware": false,
          "Relation Name": "allstat_test",
          "Alias": "allstat_test",
          "Startup Cost": 0.00,
          "Total Cost": 355.00,
          "Plan Rows": 32100,
          "Plan Width": 4,
          "Actual Startup Time": 0.000,
          "Actual Total Time": 0.010,
          "Actual Rows": 0,
          "Actual Loops": 1,
          "Allstat": [
            {
              "Segment index": 0,
              "Time To First Result": "0.307",
              "Time To Total Result": "0.010",
              "Tuples": 0.0
            },
            {
              "Segment index": 1,
              "Time To First Result": "0.304",
              "Time To Total Result": "0.010",
              "Tuples": 0.0
            },
            {
              "Segment index": 2,
              "Time To First Result": "0.252",
              "Time To Total Result": "0.008",
              "Tuples": 0.0
            }
          ]
        }
      ]
    },
    "Optimizer": "Postgres-based planner",
    "Planning Time": 0.179,
    "Triggers": [
    ],
    "Slice statistics": [
      {
        "Slice": 0,
        "Executor Memory": 36488
      },
      {
        "Slice": 1,
        "Executor Memory": {
          "Average": 36360,
          "Workers": 3,
          "Maximum Memory Used": 36360
        }
      }
    ],
    "Statement statistics": {
      "Memory used": 128000
    },
    "Execution Time": 0.489
  }
]
(1 row)
reset gp_enable_explain_allstat;
-- Cleanup
DROP TABLE boxes;
DROP TABLE apples;
DROP TABLE box_locations;
DROP TABLE jsonexplaintest;
DROP TABLE jit_explain_output;
DROP TABLE test_src_tbl;
DROP TABLE test_hashagg_spill;
DROP TABLE test_hashagg_groupingsets;
DROP TABLE stat_io_timing;
DROP TABLE allstat_test;
