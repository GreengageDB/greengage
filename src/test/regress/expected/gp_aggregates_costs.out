-- start_matchsubs
-- m/\(cost=.*\)/
-- s/\(cost=.*\)//
-- end_matchsubs
create table cost_agg_t1(a int, b int, c int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greengage Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
insert into cost_agg_t1 select i, random() * 99999, i % 2000 from generate_series(1, 1000000) i;
create table cost_agg_t2 as select * from cost_agg_t1 with no data;
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column(s) named 'a' as the Greengage Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
insert into cost_agg_t2 select i, random() * 99999, i % 300000 from generate_series(1, 1000000) i;
analyze cost_agg_t1;
analyze cost_agg_t2;
--
-- Test planner's decisions on aggregates when only little memory is available.
--
set statement_mem= '1800 kB';
-- There are only 2000 distinct values of 'c' in the table, which fits
-- comfortably in an in-memory hash table.
explain select avg(b) from cost_agg_t1 group by c;
                                               QUERY PLAN                                               
--------------------------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=5519.00..5554.00 rows=2000 width=36)
   ->  Finalize HashAggregate  (cost=5519.00..5527.33 rows=667 width=36)
         Group Key: c
         ->  Redistribute Motion 3:3  (slice2; segments: 3)  (cost=5449.00..5509.00 rows=2000 width=36)
               Hash Key: c
               ->  Partial HashAggregate  (cost=5449.00..5469.00 rows=2000 width=36)
                     Group Key: c
                     ->  Seq Scan on cost_agg_t1  (cost=0.00..3782.33 rows=333333 width=8)
 Optimizer: Postgres query optimizer
(9 rows)

-- In the other table, there are 300000 distinct values of 'c', which doesn't
-- fit in statement_mem. The planner chooses to do a single-phase agg for this.
--
-- In the single-phase plan, the aggregation is performed after redistrbuting
-- the data, which means that each node only has to process 1/(# of segments)
-- fraction of the data. That fits in memory, whereas an initial stage before
-- redistributing would not. And it would eliminate only a few rows, anyway.
explain select avg(b) from cost_agg_t2 group by c;
                                              QUERY PLAN                                               
-------------------------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=12115.67..17309.12 rows=296769 width=36)
   ->  HashAggregate  (cost=12115.67..13352.20 rows=98923 width=36)
         Group Key: c
         Planned Partitions: 8
         ->  Redistribute Motion 3:3  (slice2; segments: 3)  (cost=0.00..10449.00 rows=333333 width=8)
               Hash Key: c
               ->  Seq Scan on cost_agg_t2  (cost=0.00..3782.33 rows=333333 width=8)
 Optimizer: Postgres query optimizer
(8 rows)

-- But if there are a lot more duplicate values, the two-stage plan becomes
-- cheaper again, even though it doesn't git in memory and has to spill.
insert into cost_agg_t2 select i, random() * 99999,1 from generate_series(1, 200000) i;
analyze cost_agg_t2;
explain select avg(b) from cost_agg_t2 group by c;
                                                QUERY PLAN                                                
----------------------------------------------------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)  (cost=10113.08..11916.68 rows=103063 width=36)
   ->  Finalize HashAggregate  (cost=10113.08..10542.51 rows=34354 width=36)
         Group Key: c
         Planned Partitions: 8
         ->  Redistribute Motion 3:3  (slice2; segments: 3)  (cost=6538.00..9602.35 rows=102145 width=36)
               Hash Key: c
               ->  Partial HashAggregate  (cost=6538.00..7559.45 rows=102145 width=36)
                     Group Key: c
                     Planned Partitions: 8
                     ->  Seq Scan on cost_agg_t2  (cost=0.00..4538.00 rows=400000 width=8)
 Optimizer: Postgres query optimizer
(11 rows)

drop table cost_agg_t1;
drop table cost_agg_t2;
reset statement_mem;
-- The following case is to test GUC gp_eager_two_phase_agg for planner
-- When it is set true, planner will choose two stage agg.
create table t_planner_force_multi_stage(a int, b int) distributed randomly;
analyze t_planner_force_multi_stage;
show gp_eager_two_phase_agg;
 gp_eager_two_phase_agg 
------------------------
 off
(1 row)

-- the GUC gp_eager_two_phase_agg is default false, the table contains no data
-- so one stage agg will win.
explain (costs off) select b, sum(a) from t_planner_force_multi_stage group by b;
                         QUERY PLAN                         
------------------------------------------------------------
 Gather Motion 3:1  (slice1; segments: 3)
   ->  HashAggregate
         Group Key: b
         ->  Redistribute Motion 3:3  (slice2; segments: 3)
               Hash Key: b
               ->  Seq Scan on t_planner_force_multi_stage
 Optimizer: Postgres query optimizer
(7 rows)

set gp_eager_two_phase_agg = on;
-- when forcing two stage, it should generate two stage agg plan.
explain (costs off) select b, sum(a) from t_planner_force_multi_stage group by b;
                        QUERY PLAN                         
-----------------------------------------------------------
 Finalize HashAggregate
   Group Key: b
   ->  Gather Motion 3:1  (slice1; segments: 3)
         ->  Partial HashAggregate
               Group Key: b
               ->  Seq Scan on t_planner_force_multi_stage
 Optimizer: Postgres query optimizer
(7 rows)

reset gp_eager_two_phase_agg;
drop table t_planner_force_multi_stage;
-- Test user-defined aggregate marked safe to execute on replicated slices without motion
CREATE AGGREGATE my_unsafe_avg (float8)
(
    sfunc = float8_accum,
    stype = float8[],
    finalfunc = float8_avg,
    initcond = '{0,0,0}'
);
CREATE AGGREGATE my_safe_avg (float8)
(
    sfunc = float8_accum,
    stype = float8[],
    finalfunc = float8_avg,
    initcond = '{0,0,0}',
    repsafe = true
);
CREATE TABLE a_reptable (a int) DISTRIBUTED REPLICATED;
CREATE TABLE b_reptable (b int) DISTRIBUTED REPLICATED;
EXPLAIN INSERT INTO a_reptable(a) SELECT my_unsafe_avg(b) FROM b_reptable;
                                        QUERY PLAN                                         
-------------------------------------------------------------------------------------------
 Insert on a_reptable  (cost=1544.50..1544.55 rows=1 width=4)
   ->  Broadcast Motion 1:3  (slice1; segments: 1)  (cost=1544.50..1544.55 rows=1 width=4)
         ->  Subquery Scan on "*SELECT*"  (cost=1544.50..1544.53 rows=1 width=4)
               ->  Aggregate  (cost=1544.50..1544.51 rows=1 width=8)
                     ->  Seq Scan on b_reptable  (cost=0.00..1063.00 rows=96300 width=4)
 Optimizer: Postgres query optimizer
(6 rows)

EXPLAIN INSERT INTO a_reptable(a) SELECT my_safe_avg(b) FROM b_reptable;
                                        QUERY PLAN                                         
-------------------------------------------------------------------------------------------
 Insert on a_reptable  (cost=1544.50..1544.55 rows=1 width=4)
   ->  Broadcast Motion 1:3  (slice1; segments: 1)  (cost=1544.50..1544.55 rows=1 width=4)
         ->  Subquery Scan on "*SELECT*"  (cost=1544.50..1544.53 rows=1 width=4)
               ->  Aggregate  (cost=1544.50..1544.51 rows=1 width=8)
                     ->  Seq Scan on b_reptable  (cost=0.00..1063.00 rows=96300 width=4)
 Optimizer: Postgres query optimizer
(6 rows)

