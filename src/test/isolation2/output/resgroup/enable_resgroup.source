CREATE LANGUAGE plpythonu;
CREATE

-- start_ignore
! rmdir @cgroup_mnt_point@/cpu/gpdb;

! rmdir @cgroup_mnt_point@/cpuacct/gpdb;

! rmdir @cgroup_mnt_point@/cpuset/gpdb;

! mkdir @cgroup_mnt_point@/cpu/gpdb;

! mkdir @cgroup_mnt_point@/cpuacct/gpdb;

! mkdir @cgroup_mnt_point@/cpuset/gpdb;

-- end_ignore

-- we want to simulate a 3-segment (both master and primary) cluster with 2GB
-- memory and gp_resource_group_memory_limit=100%, suppose:
--
-- - total: the total memory on the system;
-- - nsegs: the max per-host segment count (including both master and primaries);
-- - limit: the gp_resource_group_memory_limit used for the simulation;
--
-- then we have: total * limit / nsegs = 2GB * 1.0 / 3
-- so: limit = 2GB * 1.0 / 3 * nsegs / total
--
-- with the simulation each primary segment should manage 682MB memory.
DO LANGUAGE plpythonu $$ import os import psutil 
mem = psutil.virtual_memory().total swap = psutil.swap_memory().total overcommit = int(open('/proc/sys/vm/overcommit_ratio').readline()) total = swap + mem * overcommit / 100. 
nsegs = int(plpy.execute(''' SELECT count(hostname) as nsegs FROM gp_segment_configuration WHERE preferred_role = 'p' GROUP BY hostname ORDER BY count(hostname) DESC LIMIT 1 ''')[0]['nsegs']) 
limit = (2 << 30) * 1.0 * nsegs / 3 / total os.system('gpconfig -c gp_resource_group_memory_limit -v {:f}'.format(limit)) $$;
DO

-- enable resource group and restart cluster.
-- start_ignore
! gpconfig -c gp_resource_manager -v group;
20170502:01:28:13:000367 gpconfig:sdw6:gpadmin-[INFO]:-completed successfully

! gpconfig -c gp_resource_group_cpu_limit -v 0.9;
20170731:09:42:33:021079 gpconfig:sdw8:nyu-[INFO]:-completed successfully

! gpconfig -c max_connections -v 250 -m 25;
20170731:09:42:34:021163 gpconfig:sdw8:nyu-[INFO]:-completed successfully

! gpstop -rai;
-- end_ignore

-- after the restart we need a new connection to run the queries

0: SHOW gp_resource_manager;
 gp_resource_manager 
---------------------
 group               
(1 row)

-- resource queue statistics should not crash
0: SELECT * FROM pg_resqueue_status;
 rsqname | rsqcountlimit | rsqcountvalue | rsqcostlimit | rsqcostvalue | rsqwaiters | rsqholders 
---------+---------------+---------------+--------------+--------------+------------+------------
(0 rows)
0: SELECT * FROM gp_toolkit.gp_resqueue_status;
 queueid | rsqname | rsqcountlimit | rsqcountvalue | rsqcostlimit | rsqcostvalue | rsqmemorylimit | rsqmemoryvalue | rsqwaiters | rsqholders 
---------+---------+---------------+---------------+--------------+--------------+----------------+----------------+------------+------------
(0 rows)
0: SELECT * FROM gp_toolkit.gp_resq_priority_backend;
 rqpsession | rqpcommand | rqppriority | rqpweight 
------------+------------+-------------+-----------
(0 rows)

-- verify the default settings
0: SELECT * from gp_toolkit.gp_resgroup_config;
 groupid | groupname     | concurrency | cpu_rate_limit | memory_limit | memory_shared_quota | memory_spill_ratio | memory_auditor | cpuset 
---------+---------------+-------------+----------------+--------------+---------------------+--------------------+----------------+--------
 6437    | default_group | 20          | 30             | 0            | 80                  | 0                  | vmtracker      | -1     
 6438    | admin_group   | 10          | 10             | 10           | 80                  | 0                  | vmtracker      | -1     
(2 rows)

-- by default admin_group has concurrency set to -1 which leads to
-- very small memory quota for each resgroup slot, correct it.
0: ALTER RESOURCE GROUP admin_group SET concurrency 2;
ALTER

-- explicitly set memory settings
0: ALTER RESOURCE GROUP admin_group SET memory_limit 10;
ALTER
0: ALTER RESOURCE GROUP default_group SET memory_limit 30;
ALTER
0: ALTER RESOURCE GROUP admin_group SET memory_shared_quota 80;
ALTER
0: ALTER RESOURCE GROUP default_group SET memory_shared_quota 80;
ALTER
-- in later cases we will SHOW memory_spill_ratio as first command
-- to verify that it can be correctly loaded even for bypassed commands
0: ALTER RESOURCE GROUP admin_group SET memory_spill_ratio 10;
ALTER
0: ALTER RESOURCE GROUP default_group SET memory_spill_ratio 10;
ALTER

-- session 1 set global freechunks to -5 and suspend
1: create extension gp_inject_fault;
CREATE
1: create table overuse_table(a int);
CREATE
1: select gp_inject_fault('group_overused_freechunks','suspend','', '', '', 1, -1, 0, dbid, current_setting('gp_session_id')::int) from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
1: select gp_inject_fault('group_set_overused_freechunk','skip', dbid, current_setting('gp_session_id')::int) from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
1&: select * from overuse_table;  <waiting ...>

-- session 2: alloc 1 chunk when checkDispatchResult
2: select gp_inject_fault_infinite('alloc_chunk_during_dispatch', 'skip', dbid) from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
-- execute 'set' statement to bypass query so it will use shared mem
2: SET SESSION AUTHORIZATION DEFAULT;
SET

-- reset all injected fault
2: select gp_inject_fault('alloc_chunk_during_dispatch', 'reset', dbid) from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)

2: select gp_inject_fault('group_set_overused_freechunk', 'reset', dbid) from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
2: select gp_inject_fault('group_overused_freechunks', 'reset', dbid) from gp_segment_configuration where role = 'p' and content = -1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)

1<:  <... completed>
 a 
---
(0 rows)

2: drop table overuse_table;
DROP

-- start_ignore
! gpstop -rai;
-- end_ignore
