-- when the WAL replication lag exceeds 'max_slot_wal_keep_size', the extra WAL
-- log will be removed on the primary and the replication slot will be marked as
-- obsoleted. In this case, the mirror will be marked down as well and need full
-- recovery to brought it back.

include: helpers/server_helpers.sql;
CREATE

CREATE OR REPLACE FUNCTION advance_xlog_on_seg0(num int) RETURNS void AS /*in func*/ $$ /*in func*/ DECLARE /*in func*/ i int; /*in func*/ BEGIN /*in func*/ i := 0; /*in func*/ CREATE TABLE t_dummy_switch(i int) DISTRIBUTED BY (i); /*in func*/ LOOP /*in func*/ IF i >= num THEN /*in func*/ DROP TABLE t_dummy_switch; /*in func*/ RETURN; /*in func*/ END IF; /*in func*/ PERFORM pg_switch_xlog() FROM gp_dist_random('gp_id') WHERE gp_segment_id=0; /*in func*/ INSERT INTO t_dummy_switch SELECT generate_series(1,10); /*in func*/ i := i + 1; /*in func*/ END LOOP; /*in func*/ DROP TABLE t_dummy_switch; /*in func*/ END; /*in func*/ $$ language plpgsql;
CREATE

-- On content 0 primary, retain max 128MB (2 WAL files) for
-- replication slots.  That makes it necessary to set
-- checkpoint_segments to a lower value, that is 1 WAL file.  Other
-- GUCs are needed to make the test run faster.
0U: ALTER SYSTEM SET max_slot_wal_keep_size TO 128;
ALTER
0U: ALTER SYSTEM SET checkpoint_segments TO 1;
ALTER
0U: ALTER SYSTEM SET wal_keep_segments TO 0;
ALTER
0U: ALTER SYSTEM SET gp_fts_mark_mirror_down_grace_period TO 0;
ALTER
0U: select pg_reload_conf();
 pg_reload_conf 
----------------
 t              
(1 row)
-- And on coordinator, also to make the test faster.
ALTER SYSTEM SET gp_fts_probe_retries TO 1;
ALTER
select pg_reload_conf();
 pg_reload_conf 
----------------
 t              
(1 row)

CREATE TABLE t_slot_size_limit(a int, fname text);
CREATE

----------
-- Case 1:
--
--   Verify that max_slot_wal_keep_size GUC is ignored and more WAL is
--   retained when the oldest active PREPARE record falls behind the
--   cutoff specified by the GUC.
----------

-- Suspend QD after preparing a distributed transaction, it will be
-- resumed after checkpoint.
1: SELECT gp_inject_fault('transaction_abort_after_distributed_prepared', 'suspend', dbid) FROM gp_segment_configuration WHERE content=-1 AND role='p';
 gp_inject_fault 
-----------------
 Success:        
(1 row)
-- This transaction is prepared on segments but not committed yet.  We
-- advance WAL beyond max_slot_wal_keep_size in the next few steps.
-- Checkpointer should retain WAL up to this prepare LSN, otherwise we
-- will never be able to finish this transaction.  Recording two-phase
-- commit state like this in WAL records in Greenplum specific
-- behavior.  In newer Greenplum versions and PostgreSQL, two-phase
-- state file is used to record this state, and checkpointer does not
-- need to be mindful of prepare WAL records.
3&: INSERT INTO t_slot_size_limit SELECT generate_series(101,120);  <waiting ...>
1: SELECT gp_wait_until_triggered_fault('transaction_abort_after_distributed_prepared', 1, dbid) FROM gp_segment_configuration WHERE content=-1 AND role='p';
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)

-- Walsender skip sending WAL to the mirror, build replication lag.
-- Note that this fault causes SyncRepWaitForLSN to get stuck.  We try
-- to avoid committing transactions in subsequent steps until this
-- fault is reset.
1: SELECT gp_inject_fault_infinite('walsnd_skip_send', 'skip', dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)

2: BEGIN;
BEGIN

-- Trigger the fault in walsender.  Also triggers checkpoint.
2: SELECT advance_xlog_on_seg0(1);
 advance_xlog_on_seg0 
----------------------
                      
(1 row)
1: SELECT gp_wait_until_triggered_fault('walsnd_skip_send', 1, dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)

-- Skip checkpoints on seg0.  So that when new WAL is generated in the
-- next step, checkpoints don't get triggered asynchronously.
1: SELECT gp_inject_fault_infinite('checkpoint', 'skip', dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
0U: CHECKPOINT;
CHECKPOINT
1: SELECT gp_wait_until_triggered_fault('checkpoint', 1, dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)

-- Generate more WAL on seg0 than max_slot_wal_keep_size.
2: SELECT advance_xlog_on_seg0(3);
 advance_xlog_on_seg0 
----------------------
                      
(1 row)

-- Resume checkpoints.
1: SELECT gp_inject_fault('checkpoint', 'reset', dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_inject_fault 
-----------------
 Success:        
(1 row)
-- At this point:
--    PREPARE LSN < previous checkpoint < restart_lsn
-- The checkpoint should retain WAL even when mirror has lagged behind
-- more than max_slot_wal_keep_size.
0U: CHECKPOINT;
CHECKPOINT

-- Replication slot on content 0 primary should report valid LSN
-- because checkpoint must override max_slot_wal_keep_size GUC in
-- order to retain the PREPARE record created by session 3.
0U: select restart_lsn is not null as restart_lsn_is_valid from pg_get_replication_slots();
 restart_lsn_is_valid 
----------------------
 t                    
(1 row)
-- WAL accumulated should be greater than max_slot_wal_keep_size
-- (which is set to 128MB above).
0U: select pg_xlog_location_diff(pg_current_xlog_location(), restart_lsn) / 1024 /1024 > 128 as max_slot_size_overridden from pg_get_replication_slots();
 max_slot_size_overridden 
--------------------------
 t                        
(1 row)

-- The mirror should remain up in FTS configuration.
SELECT gp_request_fts_probe_scan();
 gp_request_fts_probe_scan 
---------------------------
 t                         
(1 row)
SELECT role, preferred_role, status FROM gp_segment_configuration WHERE content = 0;
 role | preferred_role | status 
------+----------------+--------
 p    | p              | u      
 m    | m              | u      
(2 rows)

-- Unblock walsender, so that the transaction in session 3 can be
-- finished.
1: SELECT gp_inject_fault_infinite('walsnd_skip_send', 'reset', dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)

-- Unblock the session that was suspected after prepare-transaction
-- step.  It should be able to finish the transaction.
1: SELECT gp_inject_fault_infinite('transaction_abort_after_distributed_prepared', 'reset', dbid) FROM gp_segment_configuration WHERE content=-1 AND role='p';
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
3<:  <... completed>
INSERT 20
3: select count(*) from t_slot_size_limit;
 count 
-------
 20    
(1 row)
3q: ... <quitting>

----------
-- Case 2:
--
--   Verify that max_slot_wal_keep_size GUC is honored by invalidating
--   replication slot.
----------

-- Make walsender skip sending WAL to the mirror to build replication
-- lag again.
1: SELECT gp_inject_fault_infinite('walsnd_skip_send', 'skip', dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)

-- Trigger the fault in walsender.  Also triggers checkpoint.
2: SELECT advance_xlog_on_seg0(1);
 advance_xlog_on_seg0 
----------------------
                      
(1 row)
1: SELECT gp_wait_until_triggered_fault('walsnd_skip_send', 1, dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)

-- Replication slot should be valid at this time.
0U: select restart_lsn is not null as restart_lsn_is_valid from pg_get_replication_slots();
 restart_lsn_is_valid 
----------------------
 t                    
(1 row)

-- Skip checkpoints on seg0.  So that when new WAL is generated in the
-- next step, checkpoints don't get triggered asynchronously.
1: SELECT gp_inject_fault_infinite('checkpoint', 'skip', dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
0U: CHECKPOINT;
CHECKPOINT
1: SELECT gp_wait_until_triggered_fault('checkpoint', 1, dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)

-- Generate more WAL on seg0 than max_slot_wal_keep_size.
2: SELECT advance_xlog_on_seg0(3);
 advance_xlog_on_seg0 
----------------------
                      
(1 row)

-- Resume checkpoints.
1: SELECT gp_inject_fault('checkpoint', 'reset', dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_inject_fault 
-----------------
 Success:        
(1 row)
-- WAL older than max_slot_wal_keep_size should be removed by this
-- checkpoint.
0U: CHECKPOINT;
CHECKPOINT

-- Replication slot on content 0 primary should report invalid LSN
-- because the WAL files needed by it are removed by previous
-- checkpoint.
0U: select restart_lsn is not null as restart_lsn_is_valid from pg_get_replication_slots();
 restart_lsn_is_valid 
----------------------
 f                    
(1 row)

1: SELECT gp_inject_fault_infinite('walsnd_skip_send', 'reset', dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)
1: SELECT gp_request_fts_probe_scan();
 gp_request_fts_probe_scan 
---------------------------
 t                         
(1 row)
2: END;
END

-- check the mirror is down and the sync_error is set.
1: SELECT role, preferred_role, status FROM gp_segment_configuration WHERE content = 0;
 role | preferred_role | status 
------+----------------+--------
 p    | p              | u      
 m    | m              | d      
(2 rows)
1: SELECT sync_error FROM gp_stat_replication WHERE gp_segment_id = 0;
 sync_error 
------------
 walread    
(1 row)

-- Fault to check if walsender enters catchup state.
1: select gp_inject_fault('is_mirror_up', 'skip', dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_inject_fault 
-----------------
 Success:        
(1 row)

-- Wait for the mirror to make the next connection attempt.
1: SELECT gp_inject_fault('initialize_wal_sender', 'skip',  dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_inject_fault 
-----------------
 Success:        
(1 row)
-- Note that we wait until the fault is triggered twice.  Waiting
-- until the second trigger guarantees that first connection attempt
-- is fully processed and the status check that follows is accurate.
1: SELECT gp_wait_until_triggered_fault('initialize_wal_sender', 2, dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)

-- Validate that the mirror is not marked as up after replication slot
-- is obsoleted.  There used to be a bug that caused FTS to be mislead
-- by a walsender that entered catchup state but failed shorty after
-- due to the requested start point not available.  FTS marked the
-- mirror as up and turned synchronous replication on.  The following
-- query should show "num times hit" as 0, implying that the mirror's
-- status was not changed from down to up.
1: select gp_inject_fault('is_mirror_up', 'status', dbid) FROM gp_segment_configuration WHERE content=0 AND role='p';
 gp_inject_fault                                                                                                                                                                                             
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Success: fault name:'is_mirror_up' fault type:'skip' ddl statement:'' database name:'' table name:'' start occurrence:'1' end occurrence:'1' extra arg:'0' fault injection state:'set'  num times hit:'0' 
 
(1 row)

1: SELECT gp_inject_fault('all', 'reset', dbid) FROM gp_segment_configuration;
 gp_inject_fault 
-----------------
 Success:        
 Success:        
 Success:        
 Success:        
 Success:        
 Success:        
 Success:        
 Success:        
(8 rows)

0U: ALTER SYSTEM RESET max_slot_wal_keep_size;
ALTER
0U: ALTER SYSTEM RESET checkpoint_segments;
ALTER
0U: ALTER SYSTEM RESET wal_keep_segments;
ALTER
0U: ALTER SYSTEM RESET gp_fts_mark_mirror_down_grace_period;
ALTER
0U: select pg_reload_conf();
 pg_reload_conf 
----------------
 t              
(1 row)
0Uq: ... <quitting>
ALTER SYSTEM RESET gp_fts_probe_retries;
ALTER
select pg_reload_conf();
 pg_reload_conf 
----------------
 t              
(1 row)

-- do full recovery
!\retcode gprecoverseg -aF;
-- start_ignore
-- end_ignore
(exited with code 0)
select wait_until_segment_synchronized(0);
 wait_until_segment_synchronized 
---------------------------------
 OK                              
(1 row)

-- the mirror is up and the replication is back
1: SELECT role, preferred_role, status FROM gp_segment_configuration WHERE content = 0;
 role | preferred_role | status 
------+----------------+--------
 p    | p              | u      
 m    | m              | u      
(2 rows)
1: SELECT state, sync_error FROM gp_stat_replication WHERE gp_segment_id = 0;
 state     | sync_error 
-----------+------------
 streaming | none       
(1 row)

1q: ... <quitting>
2q: ... <quitting>
