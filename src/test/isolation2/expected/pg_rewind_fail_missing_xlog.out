-- Ensure that the wal files up until the one which has the last CHECKPOINT wal
-- record BEFORE the oldest replication slot restart_lsn are not recycled even
-- if more than one CHECKPOINTs are performed after that restart_lsn. Otherwise
-- gprecoverseg (based on pg_rewind) would fail due to missing wal file
CREATE EXTENSION IF NOT EXISTS gp_inject_fault;
CREATE
include: helpers/server_helpers.sql;
CREATE

CREATE OR REPLACE LANGUAGE plpythonu;
CREATE
CREATE OR REPLACE FUNCTION connectSeg(n int, port int, hostname text) RETURNS bool AS $$ import os import subprocess import time for i in range(n): try: cmd = 'PGOPTIONS="-c gp_session_role=utility" psql -h %s -p %d -d postgres -Xc "select 1"' % (hostname, port) proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True) proc.communicate() if proc.returncode == 0: return True except Exception as e: time.sleep(1) raise Exception("wait connection timeout") $$ LANGUAGE plpythonu;
CREATE

SHOW gp_keep_all_xlog;
 gp_keep_all_xlog 
------------------
 off              
(1 row)
CREATE TABLE tst_missing_tbl (a int);
CREATE
INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3

-- Make the test faster by not preserving any extra wal segment files
!\retcode gpconfig -c wal_keep_segments -v 0;
-- start_ignore

-- end_ignore
(exited with code 0)
!\retcode gpstop -ari;
(exited with code 0)

-- Test 1: Ensure that pg_rewind doesn't fail due to checkpoints inadvertently
-- recycling WAL when a former primary is marked down in configuration, while it
-- actually continues to run. The subsequent CHECKPOINT which is performed after the
-- failover on the segment being marked down should not recycle the wal file
-- which has the last common checkpoint of the target and source segment of
-- pg_rewind.

-- Run a checkpoint so that the below sqls won't cause a checkpoint
-- until an explicit checkpoint command is issued by the test.
-- checkpoint_timeout is by default 300 but the below test should be able to
-- finish in 300 seconds.
1: CHECKPOINT;
CHECKPOINT

0U: SELECT pg_switch_xlog is not null FROM pg_switch_xlog();
 ?column? 
----------
 t        
(1 row)
1: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
-- Should be not needed mostly but let's 100% ensure since pg_switch_xlog()
-- won't switch if it has been on the boundary (seldom though).
0U: SELECT pg_switch_xlog is not null FROM pg_switch_xlog();
 ?column? 
----------
 t        
(1 row)
1: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
0Uq: ... <quitting>

-- Make sure primary/mirror pair is in sync, otherwise FTS can't promote mirror
1: SELECT wait_until_all_segments_synchronized();
 wait_until_all_segments_synchronized 
--------------------------------------
 OK                                   
(1 row)
-- Mark down the primary with content 0 via fts fault injection.
1: SELECT gp_inject_fault_infinite('fts_handle_message', 'error', dbid) FROM gp_segment_configuration WHERE content = 0 AND role = 'p';
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)

-- Trigger failover and double check.
1: SELECT gp_request_fts_probe_scan();
 gp_request_fts_probe_scan 
---------------------------
 t                         
(1 row)
1: SELECT role, preferred_role from gp_segment_configuration where content = 0;
 role | preferred_role 
------+----------------
 m    | p              
 p    | m              
(2 rows)

-- Run two more checkpoints. Previously this causes the checkpoint.redo wal
-- file before the oldest replication slot LSN is recycled/removed.
0M: CHECKPOINT;
CHECKPOINT
0M: BEGIN;
BEGIN
0M: DROP TABLE tst_missing_tbl;
DROP
0M: ABORT;
ABORT
0M: CHECKPOINT;
CHECKPOINT
0Mq: ... <quitting>

-- Write something (promote adds a 'End Of Recovery' xlog that causes the
-- divergence between primary and mirror, but I add a write here so that we
-- know that a wal divergence is explicitly triggered and 100% completed.  Also
-- sanity check the tuple distribution (assumption of the test).
2: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
2: SELECT gp_segment_id, count(*) from tst_missing_tbl group by gp_segment_id;
 gp_segment_id | count 
---------------+-------
 0             | 4     
 1             | 4     
 2             | 4     
(3 rows)

-- Ensure that pg_rewind succeeds. Previously it could fail since the divergence
-- LSN wal file is missing.
!\retcode gprecoverseg -av;
-- start_ignore
20210429:15:31:23:021451 gprecoverseg:host67:pguo-[DEBUG]:-pg_rewind results: cmd had rc=1 completed=True halted=False
  stdout=''
  stderr=''
20210429:15:31:23:021451 gprecoverseg:host67:pguo-[DEBUG]:-rewind dbid: 2 failed
20210429:15:31:23:021451 gprecoverseg:host67:pguo-[WARNING]:-
20210429:15:31:23:021451 gprecoverseg:host67:pguo-[WARNING]:-Incremental recovery failed for dbid 2. You must use gprecoverseg -F to recover the segment.
-- end_ignore
(exited with code 0)
-- In case it fails it should not affect subsequent testing.
!\retcode gprecoverseg -aF;
(exited with code 0)
2: SELECT wait_until_all_segments_synchronized();
 wait_until_all_segments_synchronized 
--------------------------------------
 OK                                   
(1 row)

-- Test 2: Ensure that pg_rewind doesn't fail due to checkpoints inadvertently
-- recycling WAL when a former primary was abnormally shutdown. In the case the
-- target segment was abnormally shutdown, pg_rewind starts and then stops
-- the target segment as single-user mode postgres to ensure clean shutdown which
-- causes two checkpoints. The two newer checkpoints introduced by pg_rewind
-- should not recycle the wal file that has the last common checkpoint of the
-- target and source segment of pg_rewind.

-- See previous comment for why.
3: CHECKPOINT;
CHECKPOINT

1U: SELECT pg_switch_xlog is not null FROM pg_switch_xlog();
 ?column? 
----------
 t        
(1 row)
3: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
1U: SELECT pg_switch_xlog is not null FROM pg_switch_xlog();
 ?column? 
----------
 t        
(1 row)
3: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
1U: SELECT pg_switch_xlog is not null FROM pg_switch_xlog();
 ?column? 
----------
 t        
(1 row)
3: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
-- Should be not needed mostly but let's 100% ensure since pg_switch_xlog()
-- won't switch if it is on the boundary already (seldom though).
1U: SELECT pg_switch_xlog is not null FROM pg_switch_xlog();
 ?column? 
----------
 t        
(1 row)
3: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3

-- Hang at checkpointer before writing checkpoint xlog.
3: SELECT gp_inject_fault('checkpoint_after_redo_calculated', 'suspend', dbid) FROM gp_segment_configuration WHERE role='p' AND content = 1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
1U&: CHECKPOINT;  <waiting ...>
3: SELECT gp_wait_until_triggered_fault('checkpoint_after_redo_calculated', 1, dbid) FROM gp_segment_configuration WHERE role='p' AND content = 1;
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)

-- Stop the primary immediately and promote the mirror.
3: SELECT pg_ctl(datadir, 'stop', 'immediate') FROM gp_segment_configuration WHERE role='p' AND content = 1;
 pg_ctl 
--------
 OK     
(1 row)
3: SELECT gp_request_fts_probe_scan();
 gp_request_fts_probe_scan 
---------------------------
 t                         
(1 row)
-- Wait for the segment promotion finished and accept the connection
3: select connectSeg(600,port,hostname) from gp_segment_configuration where content = 1 and role = 'p';
 connectseg 
------------
 t          
(1 row)
-- Wait for the end of recovery CHECKPOINT completed after the mirror was promoted
3: SELECT gp_inject_fault('checkpoint_after_redo_calculated', 'skip', dbid) FROM gp_segment_configuration WHERE role='p' AND content = 1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
3: SELECT gp_wait_until_triggered_fault('checkpoint_after_redo_calculated', 1, dbid) FROM gp_segment_configuration WHERE role = 'p' AND content = 1;
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)
3: SELECT gp_inject_fault('checkpoint_after_redo_calculated', 'reset', dbid) FROM gp_segment_configuration WHERE role = 'p' AND content = 1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
3: SELECT role, preferred_role from gp_segment_configuration where content = 1;
 role | preferred_role 
------+----------------
 m    | p              
 p    | m              
(2 rows)

4: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
4: SELECT gp_segment_id, count(*) from tst_missing_tbl group by gp_segment_id;
 gp_segment_id | count 
---------------+-------
 2             | 9     
 0             | 9     
 1             | 9     
(3 rows)

-- CHECKPOINT should fail now.
1U<:  <... completed>
server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.
1Uq: ... <quitting>

-- Ensure that pg_rewind succeeds. For unclean shutdown, there are two
-- checkpoints are introduced in pg_rewind when running single-user mode postgres
-- (one is the checkpoint after crash recovery and another is the shutdown
-- checkpoint) and previously the checkpoints clean up the wal files that
-- include the previous checkpoint (before divergence LSN) for pg_rewind and
-- thus makes gprecoverseg (pg_rewind) fail.
!\retcode gprecoverseg -av;
-- start_ignore
-- end_ignore
(exited with code 0)
-- In case it fails it should not affect subsequent testing.
!\retcode gprecoverseg -aF;
-- start_ignore
-- end_ignore
(exited with code 0)
4: SELECT wait_until_all_segments_synchronized();
 wait_until_all_segments_synchronized 
--------------------------------------
 OK                                   
(1 row)

-- Test 3: Ensure that pg_rewind doesn't fail due to checkpoints inadvertently
-- recycling WAL when a former primary is marked down in configuration, while it
-- actually continued running for a while before it was cleanly shutdown. The wal
-- file on the target segment which has the last common checkpoint of the target
-- and source segment of pg_rewind should survive the subsequent CHECKPOINTs
-- performed on the target segment even if the segment was cleanly shutdown
-- and started again after the failover.

-- Run a checkpoint so that the below sqls won't cause a checkpoint
-- until an explicit checkpoint command is issued by the test.
-- checkpoint_timeout is by default 300 but the below test should be able to
-- finish in 300 seconds.
1: CHECKPOINT;
CHECKPOINT

0U: SELECT pg_switch_xlog is not null FROM pg_switch_xlog();
 ?column? 
----------
 t        
(1 row)
1: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
-- Should be not needed mostly but let's 100% ensure since pg_switch_xlog()
-- won't switch if it has been on the boundary (seldom though).
0U: SELECT pg_switch_xlog is not null FROM pg_switch_xlog();
 ?column? 
----------
 t        
(1 row)
1: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
0Uq: ... <quitting>

-- Make sure primary/mirror pair is in sync, otherwise FTS can't promote mirror
1: SELECT wait_until_all_segments_synchronized();
 wait_until_all_segments_synchronized 
--------------------------------------
 OK                                   
(1 row)
-- Mark down the primary with content 0 via fts fault injection.
1: SELECT gp_inject_fault_infinite('fts_handle_message', 'error', dbid) FROM gp_segment_configuration WHERE content = 0 AND role = 'p';
 gp_inject_fault_infinite 
--------------------------
 Success:                 
(1 row)

-- Trigger failover and double check.
1: SELECT gp_request_fts_probe_scan();
 gp_request_fts_probe_scan 
---------------------------
 t                         
(1 row)
1: SELECT role, preferred_role from gp_segment_configuration where content = 0;
 role | preferred_role 
------+----------------
 m    | m              
 p    | p              
(2 rows)

-- Run two more checkpoints. Previously this causes the checkpoint.redo wal
-- file before the oldest replication slot LSN is recycled/removed.
0M: BEGIN;
BEGIN
0M: DROP TABLE tst_missing_tbl;
DROP
0M: ABORT;
ABORT
0M: CHECKPOINT;
CHECKPOINT
0M: BEGIN;
BEGIN
0M: DROP TABLE tst_missing_tbl;
DROP
0M: ABORT;
ABORT
0M: CHECKPOINT;
CHECKPOINT

-- Clean shutdown. Clean shutdown performs CHECKPOINT
2: SELECT pg_ctl(datadir, 'stop', 'fast') from gp_segment_configuration where role = 'm' and content = 0;
 pg_ctl 
--------
 OK     
(1 row)
0Mq: ... <quitting>

-- Start again. Start from a clean shutdown does not perform CHECKPOINT
2: SELECT pg_ctl_start(datadir, port) from gp_segment_configuration where role = 'm' and content = 0;
 pg_ctl_start     
------------------
 server starting
 
(1 row)
-- Wait for a few seconds for the mirror to be ready to accept connections
2: SELECT pg_sleep(5);
 pg_sleep 
----------
          
(1 row)

-- Perform CHECKPOINT. Previously this causes the checkpoint.redo wal
-- file before the oldest replication slot LSN is recycled/removed.
0M: CHECKPOINT;
CHECKPOINT

-- Write something (promote adds a 'End Of Recovery' xlog that causes the
-- divergence between primary and mirror, but I add a write here so that we
-- know that a wal divergence is explicitly triggered and 100% completed.  Also
-- sanity check the tuple distribution (assumption of the test).
2: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
2: SELECT gp_segment_id, count(*) from tst_missing_tbl group by gp_segment_id;
 gp_segment_id | count 
---------------+-------
 1             | 12    
 2             | 12    
 0             | 12    
(3 rows)

-- Ensure that pg_rewind succeeds. Previously it could fail since the divergence
-- LSN wal file is missing.
!\retcode gprecoverseg -av;
-- start_ignore
-- end_ignore
(exited with code 0)
-- In case it fails it should not affect subsequent testing.
!\retcode gprecoverseg -aF;
-- start_ignore



-- end_ignore
(exited with code 0)
2: SELECT wait_until_all_segments_synchronized();
 wait_until_all_segments_synchronized 
--------------------------------------
 OK                                   
(1 row)

-- Test 4: Ensure that pg_rewind doesn't fail due to checkpoints inadvertently
-- recycling WAL when a former primary was abnormally shutdown and it wrote
-- a CHECKPOINT record locally but the record did not make to the wal receiver.
-- In the case the target segment was abnormally shutdown, pg_rewind starts and
-- then stops the target segment as single-user mode postgres to ensure clean shutdown
-- which causes two checkpoints. The two newer checkpoints introduced by pg_rewind
-- plus the checkpoint during the unclean shutdown should not recycle the wal
-- file that has the last common checkpoint of the target and source segment of
-- pg_rewind.

-- See previous comment for why.
3: CHECKPOINT;
CHECKPOINT
1U: SELECT pg_switch_xlog is not null FROM pg_switch_xlog();
 ?column? 
----------
 t        
(1 row)
3: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
-- Should be not needed mostly but let's 100% ensure since pg_switch_xlog()
-- won't switch if it is on the boundary already (seldom though).
1U: SELECT pg_switch_xlog is not null FROM pg_switch_xlog();
 ?column? 
----------
 t        
(1 row)
3: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3

-- Have primary/mirror pair in sync before suspending the wal sender.
3: SELECT wait_until_all_segments_synchronized();
 wait_until_all_segments_synchronized 
--------------------------------------
 OK                                   
(1 row)

-- Hang the wal sender before writing checkpoint wal record.
3: SELECT gp_inject_fault('wal_sender_loop', 'suspend', dbid) FROM gp_segment_configuration WHERE role='p' AND content = 1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
3: SELECT gp_wait_until_triggered_fault('wal_sender_loop', 1, dbid) FROM gp_segment_configuration WHERE role='p' AND content = 1;
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)

-- Checkpoint and make sure the CHECKPOINT record is written on disk
3: SELECT gp_inject_fault('checkpoint_control_file_updated', 'suspend', dbid) FROM gp_segment_configuration WHERE role='p' AND content = 1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
1U&: CHECKPOINT;  <waiting ...>
3: SELECT gp_wait_until_triggered_fault('checkpoint_control_file_updated', 1, dbid) FROM gp_segment_configuration WHERE role='p' AND content = 1;
 gp_wait_until_triggered_fault 
-------------------------------
 Success:                      
(1 row)

-- Stop the primary immediately and promote the mirror.
3: SELECT pg_ctl(datadir, 'stop', 'immediate') FROM gp_segment_configuration WHERE role='p' AND content = 1;
 pg_ctl 
--------
 OK     
(1 row)
3: SELECT gp_request_fts_probe_scan();
 gp_request_fts_probe_scan 
---------------------------
 t                         
(1 row)

-- Wait for the segment promotion finished and accept the connection
3: select connectSeg(600,port,hostname) from gp_segment_configuration where content = 1 and role = 'p';
 connectseg 
------------
 t          
(1 row)
-- Reset faults and confirm FTS configuration
3: SELECT gp_inject_fault('wal_sender_loop', 'reset', dbid) FROM gp_segment_configuration WHERE role='p' AND content = 1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
3: SELECT gp_inject_fault('checkpoint_control_file_updated', 'reset', dbid) FROM gp_segment_configuration WHERE role = 'p' AND content = 1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
3: SELECT role, preferred_role from gp_segment_configuration where content = 1;
 role | preferred_role 
------+----------------
 m    | m              
 p    | p              
(2 rows)

-- Write something on the current primary
4: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
4: SELECT gp_segment_id, count(*) from tst_missing_tbl group by gp_segment_id;
 gp_segment_id | count 
---------------+-------
 0             | 15    
 2             | 15    
 1             | 15    
(3 rows)

-- CHECKPOINT should fail now.
1U<:  <... completed>
server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.
1Uq: ... <quitting>

-- Ensure that pg_rewind succeeds. For unclean shutdown, there are two
-- checkpoints are introduced in pg_rewind when running single-user mode postgres
-- (one is the checkpoint after crash recovery and another is the shutdown
-- checkpoint) and previously the checkpoints clean up the wal files that
-- include the previous checkpoint (before divergence LSN) for pg_rewind and
-- thus makes gprecoverseg (pg_rewind) fail.
!\retcode gprecoverseg -av;
-- start_ignore
-- end_ignore
(exited with code 0)
-- In case it fails it should not affect subsequent testing.
!\retcode gprecoverseg -aF;
-- start_ignore
-- end_ignore
(exited with code 0)
4: SELECT wait_until_all_segments_synchronized();
 wait_until_all_segments_synchronized 
--------------------------------------
 OK                                   
(1 row)

-- Test 5: Ensure that replication slot's restart_lsn advances and WAL files
-- that are older than the minimum restart_lsn get removed/recycled.

-- Create an unlogged table on the primary that remembers replication slot's last restart_lsn and number of WAL files.
1U: CREATE UNLOGGED TABLE unlogged_wal_retention_test(restart_lsn_before pg_lsn, wal_count_before int);
CREATE
1U: INSERT INTO unlogged_wal_retention_test SELECT (select restart_lsn FROM pg_replication_slots WHERE slot_name = 'internal_wal_replication_slot') as restart_lsn_before, (select count(*) from pg_ls_dir('./pg_xlog')) as wal_count_before;
INSERT 1
5: CHECKPOINT;
CHECKPOINT
-- Replication slot's restart_lsn should advance to the checkpoint's redo location.
1U: SELECT pg_xlog_location_diff(restart_lsn, restart_lsn_before) > 0 from pg_replication_slots, unlogged_wal_retention_test WHERE slot_name = 'internal_wal_replication_slot';
 ?column? 
----------
 t        
(1 row)
1U: SELECT pg_xlog_location_diff(restart_lsn, (select pg_controldata_redo_lsn(setting) FROM pg_settings WHERE name = 'data_directory')) = 0 from pg_replication_slots WHERE slot_name = 'internal_wal_replication_slot';
 ?column? 
----------
 t        
(1 row)
-- Some old WALs should be removed
1U: select ((select count(*)::int from pg_ls_dir('./pg_xlog')) - wal_count_before) < 0 FROM unlogged_wal_retention_test;
 ?column? 
----------
 t        
(1 row)
-- Record the restart_lsn and the WAL file count.
1U: UPDATE unlogged_wal_retention_test SET restart_lsn_before = (SELECT restart_lsn from pg_replication_slots WHERE slot_name = 'internal_wal_replication_slot'), wal_count_before = (select count(*) from pg_ls_dir('./pg_xlog'));
UPDATE 1
-- Write some records to a newer WAL file.
1U: SELECT pg_switch_xlog is not null FROM pg_switch_xlog();
 ?column? 
----------
 t        
(1 row)
5: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
-- Replication slot's restart_lsn should NOT change regardless mirror has received more wals.
1U: select pg_xlog_location_diff(restart_lsn, restart_lsn_before) = 0 FROM pg_replication_slots, unlogged_wal_retention_test WHERE slot_name = 'internal_wal_replication_slot';
 ?column? 
----------
 t        
(1 row)
--- Record the WAL file count.
1U: UPDATE unlogged_wal_retention_test SET wal_count_before = (select count(*) from pg_ls_dir('./pg_xlog'));
UPDATE 1

-- Hang the wal sender before writing more wals.
5: SELECT gp_inject_fault('wal_sender_loop', 'suspend', dbid) FROM gp_segment_configuration WHERE role='p' AND content = 1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
-- Perform checkpoint. This is to mimic walreciver's confirmed received lsn is behind the redo lsn.
1U: CHECKPOINT;
CHECKPOINT

-- Replication slot's restart_lsn should NOT change regardless a new checkpoints was performed.
1U: select pg_xlog_location_diff(restart_lsn, restart_lsn_before) = 0 FROM pg_replication_slots, unlogged_wal_retention_test WHERE slot_name = 'internal_wal_replication_slot';
 ?column? 
----------
 t        
(1 row)
-- Replication slot's restart_lsn should be smaller than the checkpoint's redo location.
1U: SELECT pg_xlog_location_diff(restart_lsn, (select pg_controldata_redo_lsn(setting) FROM pg_settings WHERE name = 'data_directory')) < 0 from pg_replication_slots WHERE slot_name = 'internal_wal_replication_slot';
 ?column? 
----------
 t        
(1 row)

-- Resume walsender.
5: SELECT gp_inject_fault('wal_sender_loop', 'reset', dbid) FROM gp_segment_configuration WHERE role='p' AND content = 1;
 gp_inject_fault 
-----------------
 Success:        
(1 row)
-- Perform transaction to make sure wals are in sync.
5: INSERT INTO tst_missing_tbl values(2),(1),(5);
INSERT 3
-- Replication slot's restart_lsn should now advance to the checkpoint's redo location..
1U: SELECT pg_xlog_location_diff(restart_lsn, restart_lsn_before) > 0 from pg_replication_slots, unlogged_wal_retention_test WHERE slot_name = 'internal_wal_replication_slot';
 ?column? 
----------
 t        
(1 row)
-- Trigger old WAL removal.
1U: CHECKPOINT;
CHECKPOINT
-- And the WAL file(s) older than the redo lsn that we previously kept should now be removed.
1U: select ((select count(*)::int from pg_ls_dir('./pg_xlog')) - wal_count_before) < 0 FROM unlogged_wal_retention_test;
 ?column? 
----------
 t        
(1 row)
1U: SELECT pg_xlog_location_diff(restart_lsn, (select pg_controldata_redo_lsn(setting) FROM pg_settings WHERE name = 'data_directory')) = 0 from pg_replication_slots WHERE slot_name = 'internal_wal_replication_slot';
 ?column? 
----------
 t        
(1 row)

-- Cleanup
1U: DROP TABLE unlogged_wal_retention_test;
DROP
1Uq: ... <quitting>

5: DROP TABLE tst_missing_tbl;
DROP
5: DROP FUNCTION connectSeg(n int, port int, hostname text);
DROP
!\retcode gpconfig -r wal_keep_segments;
-- start_ignore
20230111:09:22:43:674963 gpconfig:station3:pivotal-[INFO]:-completed successfully with parameters '-r wal_recycle'

-- end_ignore
(exited with code 0)
!\retcode gpstop -ari;
-- start_ignore
-- end_ignore
(exited with code 0)
